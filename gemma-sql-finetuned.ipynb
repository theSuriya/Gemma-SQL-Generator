{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip install -q -U datasets==2.16.1"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip install -q -U bitsandbytes==0.42.0\n","!pip install -q -U peft==0.8.2\n","!pip install -q -U trl==0.7.10\n","!pip install -q -U accelerate==0.27.1\n","!pip install -q -U transformers==4.38.0"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# !pip install huggingface_hub --q"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from huggingface_hub import notebook_login\n","notebook_login()# run and pass the HuggingFace API Token"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["checkpoint = \"suriya7/Gemma2B-Finetuned-Sql-Generator\""]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_use_double_quant=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.bfloat16\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["model_id = checkpoint\n","\n","\n","model = AutoModelForCausalLM.from_pretrained(model_id,quantization_config=bnb_config, device_map={\"\":0})\n","tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token=True)"]},{"cell_type":"markdown","metadata":{},"source":["## Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import torch\n","\n","prompt_template = \"\"\"\n","<start_of_turn>user\n","You are an intelligent AI specialized in generating SQL queries.\n","Your task is to assist users in formulating SQL queries to retrieve specific information from a database.\n","Please provide the SQL query corresponding to the given prompt and context:\n","\n","Prompt:\n","find the price of laptop\n","\n","Context:\n","CREATE TABLE products (\n","    product_id INT,\n","    product_name VARCHAR(100),\n","    category VARCHAR(50),\n","    price DECIMAL(10, 2),\n","    stock_quantity INT\n",");\n","\n","INSERT INTO products (product_id, product_name, category, price, stock_quantity) \n","VALUES \n","    (1, 'Smartphone', 'Electronics', 599.99, 100),\n","    (2, 'Laptop', 'Electronics', 999.99, 50),\n","    (3, 'Headphones', 'Electronics', 99.99, 200),\n","    (4, 'T-shirt', 'Apparel', 19.99, 300),\n","    (5, 'Jeans', 'Apparel', 49.99, 150);<end_of_turn>\n","<start_of_turn>model\n","\"\"\"\n","\n","prompt = prompt_template\n","encodeds = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True).input_ids\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","inputs = encodeds.to(device)\n","\n","\n","# Increase max_new_tokens if needed\n","generated_ids = model.generate(inputs, max_new_tokens=1000, do_sample=True, temperature = 0.7,pad_token_id=tokenizer.eos_token_id)\n","ans = ''\n","for i in tokenizer.decode(generated_ids[0], skip_special_tokens=True).split('<end_of_turn>')[:2]:\n","    ans += i\n","\n","# Extract only the model's answer\n","model_answer = ans.split(\"model\")[1].strip()\n","print(model_answer)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from datasets import load_dataset, DatasetDict\n","from datasets import concatenate_datasets\n","\n","\n","dataset = load_dataset('gretelai/synthetic_text_to_sql')#load the dataset that you want to fine tune"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["dataset"]},{"cell_type":"markdown","metadata":{},"source":["### Prompt Format for Google Gemma Model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["prompt_template = \"\"\"\n","<start_of_turn>user\n","Answer the following question in a concise and informative manner:\n"," \n","Explain why the sky is blue<end_of_turn>\n","<start_of_turn>model\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def generate_prompt(data_point):\n","    \"\"\"Generate input text based on a prompt, task instruction, (context info), and answer.\n","\n","    :param data_point: dict: Data point\n","    :return: dict: Data point with the added \"prompt\" field\n","    \"\"\"\n","    prompt_template = \"\"\"\n","    <start_of_turn>user\n","    You are an intelligent AI specialized in generating SQL queries.\n","    Your task is to assist users in formulating SQL queries to retrieve specific information from a database.\n","    Please provide the SQL query corresponding to the given prompt and context:\n","\n","    Prompt:\n","    {sql_prompt}\n","\n","    Context:\n","    {sql_context}<end_of_turn>\n","    <start_of_turn>model\n","    {sql}<end_of_turn>\n","    \"\"\"\n","\n","    prompt_text = prompt_template.format(sql_prompt=data_point[\"sql_prompt\"],sql_context=data_point[\"sql_context\"],sql=data_point[\"sql\"])\n","    data_point[\"prompt\"] = prompt_text\n","\n","    return data_point\n","\n","# Add the \"prompt\" column to the dataset\n","dataset = dataset['train'].map(generate_prompt)\n","\n","# Print the updated dataset\n","dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(dataset['prompt'][99])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dataset = dataset.shuffle(seed=2000)  # Shuffle dataset here\n","dataset = dataset.map(lambda samples: tokenizer(samples[\"prompt\"]), batched=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["dataset = dataset.train_test_split(test_size=0.2)\n","train_data = dataset[\"train\"]\n","test_data = dataset[\"test\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test_data"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n","model.gradient_checkpointing_enable()\n","model = prepare_model_for_kbit_training(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import bitsandbytes as bnb\n","def find_all_linear_names(model):\n","  cls = bnb.nn.Linear4bit #if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)\n","  lora_module_names = set()\n","  for name, module in model.named_modules():\n","    if isinstance(module, cls):\n","      names = name.split('.')\n","      lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n","    if 'lm_head' in lora_module_names: # needed for 16-bit\n","      lora_module_names.remove('lm_head')\n","  return list(lora_module_names)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["modules = find_all_linear_names(model)\n","print(modules)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from peft import LoraConfig, get_peft_model\n","\n","lora_config = LoraConfig(\n","    r=100,\n","    lora_alpha=32,\n","    target_modules=modules,\n","    lora_dropout=0.05,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\"\n",")\n","\n","model = get_peft_model(model, lora_config)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["trainable, total = model.get_nb_trainable_parameters()\n","print(f\"Trainable: {trainable} | total: {total} | Percentage: {trainable/total*100:.4f}%\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from trl import SFTTrainer\n","import transformers\n","import torch\n","\n","tokenizer.pad_token = tokenizer.eos_token\n","torch.cuda.empty_cache()\n","\n","trainer = SFTTrainer(\n","    model,\n","    train_dataset=dataset['train'],\n","    dataset_text_field=\"prompt\",\n","    args=transformers.TrainingArguments(\n","        learning_rate=2e-4,\n","        output_dir=\"Gemma\",\n","        per_device_train_batch_size=1,\n","        num_train_epochs=1,\n","        logging_strategy=\"steps\",\n","        save_strategy=\"steps\",\n","        logging_steps=10,\n","        save_steps=100000,\n","        weight_decay=0.01,\n","        adam_beta1=0.9,\n","        adam_beta2=0.98,\n","        adam_epsilon=1e-8,\n","        warmup_steps=0.03,\n","        fp16=True,\n","        seed=42,\n","        save_total_limit=1,\n","    ),data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model.config.use_cache = False\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["new_model = \"gemma-model\""]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["trainer.model.save_pretrained(\"gemma-model\")# save the model"]},{"cell_type":"markdown","metadata":{},"source":["### Save the model weights and adapters"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["base_model = AutoModelForCausalLM.from_pretrained(\n","    checkpoint,\n","    low_cpu_mem_usage=True,\n","    return_dict=True,\n","    torch_dtype=torch.float16,\n","    device_map={\"\": 0},\n",")\n","merged_model= PeftModel.from_pretrained(base_model, new_model)\n","merged_model= merged_model.merge_and_unload()\n","\n","# Save the merged model\n","merged_model.save_pretrained(\"merged_model\",safe_serialization=True)\n","tokenizer.save_pretrained(\"merged_model\")\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"right\""]},{"cell_type":"markdown","metadata":{},"source":["### Test With New Samples"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["prompt_template = \"\"\"\n","<start_of_turn>user\n","You are an intelligent AI specialized in generating SQL queries.\n","Your task is to assist users in formulating SQL queries to retrieve specific information from a database.\n","Please provide the SQL query corresponding to the given prompt and context:\n","\n","Prompt:\n","What is the total production of oil from the onshore fields in the Beaufort Sea?\n","\n","Context:\n","CREATE TABLE beaufort_sea_oil_production (field VARCHAR(255), year INT, production FLOAT);<end_of_turn>\n","<start_of_turn>model\n","    \"\"\"\n","\n","prompt = prompt_template\n","encodeds = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True).input_ids\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","merged_model.to(device)\n","inputs = encodeds.to(device)\n","\n","\n","# Increase max_new_tokens if needed\n","generated_ids = merged_model.generate(inputs, max_new_tokens=1000, do_sample=True, temperature = 0.7,pad_token_id=tokenizer.eos_token_id)\n","ans = ''\n","for i in tokenizer.decode(generated_ids[0], skip_special_tokens=True).split('<end_of_turn>')[:2]:\n","    ans += i\n","\n","# Extract only the model's answer\n","model_answer = ans.split(\"model\")[1].strip()\n","print(model_answer)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30674,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
