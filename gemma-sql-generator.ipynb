{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q -U datasets==2.16.1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -q -U bitsandbytes==0.42.0\n!pip install -q -U peft==0.8.2\n!pip install -q -U trl==0.7.10\n!pip install -q -U accelerate==0.27.1\n!pip install -q -U transformers==4.38.0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install huggingface_hub --q","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()# run and pass the HuggingFace API Token","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint = \"suriya7/Gemma2B-Finetuned-Sql-Generator\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_id = checkpoint\n\n\nmodel = AutoModelForCausalLM.from_pretrained(model_idquantization_config=bnb_config, device_map={\"\":0})\ntokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token=True)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference","metadata":{}},{"cell_type":"code","source":"import torch\n\nprompt_template = \"\"\"\n<start_of_turn>user\nYou are an intelligent AI specialized in generating SQL queries.\nYour task is to assist users in formulating SQL queries to retrieve specific information from a database.\nPlease provide the SQL query corresponding to the given prompt and context:\n\nPrompt:\nfind the price of laptop\n\nContext:\nCREATE TABLE products (\n    product_id INT,\n    product_name VARCHAR(100),\n    category VARCHAR(50),\n    price DECIMAL(10, 2),\n    stock_quantity INT\n);\n\nINSERT INTO products (product_id, product_name, category, price, stock_quantity) \nVALUES \n    (1, 'Smartphone', 'Electronics', 599.99, 100),\n    (2, 'Laptop', 'Electronics', 999.99, 50),\n    (3, 'Headphones', 'Electronics', 99.99, 200),\n    (4, 'T-shirt', 'Apparel', 19.99, 300),\n    (5, 'Jeans', 'Apparel', 49.99, 150);<end_of_turn>\n<start_of_turn>model\n\"\"\"\n\nprompt = prompt_template\nencodeds = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True).input_ids\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\ninputs = encodeds.to(device)\n\n\n# Increase max_new_tokens if needed\ngenerated_ids = model.generate(inputs, max_new_tokens=1000, do_sample=True, temperature = 0.7,pad_token_id=tokenizer.eos_token_id)\nans = ''\nfor i in tokenizer.decode(generated_ids[0], skip_special_tokens=True).split('<end_of_turn>')[:2]:\n    ans += i\n\n# Extract only the model's answer\nmodel_answer = ans.split(\"model\")[1].strip()\nprint(model_answer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset, DatasetDict\nfrom datasets import concatenate_datasets\n\n\ndataset = load_dataset('gretelai/synthetic_text_to_sql')#load the dataset that you want to fine tune","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Prompt Format for Google Gemma Model","metadata":{}},{"cell_type":"code","source":"prompt_template = \"\"\"\n<start_of_turn>user\nAnswer the following question in a concise and informative manner:\n \nExplain why the sky is blue<end_of_turn>\n<start_of_turn>model\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_prompt(data_point):\n    \"\"\"Generate input text based on a prompt, task instruction, (context info), and answer.\n\n    :param data_point: dict: Data point\n    :return: dict: Data point with the added \"prompt\" field\n    \"\"\"\n    prompt_template = \"\"\"\n    <start_of_turn>user\n    You are an intelligent AI specialized in generating SQL queries.\n    Your task is to assist users in formulating SQL queries to retrieve specific information from a database.\n    Please provide the SQL query corresponding to the given prompt and context:\n\n    Prompt:\n    {sql_prompt}\n\n    Context:\n    {sql_context}<end_of_turn>\n    <start_of_turn>model\n    {sql}<end_of_turn>\n    \"\"\"\n\n    prompt_text = prompt_template.format(sql_prompt=data_point[\"sql_prompt\"],sql_context=data_point[\"sql_context\"],sql=data_point[\"sql\"])\n    data_point[\"prompt\"] = prompt_text\n\n    return data_point\n\n# Add the \"prompt\" column to the dataset\ndataset = dataset['train'].map(generate_prompt)\n\n# Print the updated dataset\ndataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(dataset['prompt'][99])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = dataset.shuffle(seed=2000)  # Shuffle dataset here\ndataset = dataset.map(lambda samples: tokenizer(samples[\"prompt\"]), batched=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = dataset.train_test_split(test_size=0.2)\ntrain_data = dataset[\"train\"]\ntest_data = dataset[\"test\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\nmodel.gradient_checkpointing_enable()\nmodel = prepare_model_for_kbit_training(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import bitsandbytes as bnb\ndef find_all_linear_names(model):\n  cls = bnb.nn.Linear4bit #if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)\n  lora_module_names = set()\n  for name, module in model.named_modules():\n    if isinstance(module, cls):\n      names = name.split('.')\n      lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n    if 'lm_head' in lora_module_names: # needed for 16-bit\n      lora_module_names.remove('lm_head')\n  return list(lora_module_names)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"modules = find_all_linear_names(model)\nprint(modules)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model\n\nlora_config = LoraConfig(\n    r=100,\n    lora_alpha=32,\n    target_modules=modules,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\nmodel = get_peft_model(model, lora_config)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainable, total = model.get_nb_trainable_parameters()\nprint(f\"Trainable: {trainable} | total: {total} | Percentage: {trainable/total*100:.4f}%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from trl import SFTTrainer\nimport transformers\nimport torch\n\ntokenizer.pad_token = tokenizer.eos_token\ntorch.cuda.empty_cache()\n\ntrainer = SFTTrainer(\n    model,\n    train_dataset=dataset['train'],\n    dataset_text_field=\"prompt\",\n    args=transformers.TrainingArguments(\n        learning_rate=2e-4,\n        output_dir=\"Gemma\",\n        per_device_train_batch_size=1,\n        num_train_epochs=1,\n        logging_strategy=\"steps\",\n        save_strategy=\"steps\",\n        logging_steps=10,\n        save_steps=100000,\n        weight_decay=0.01,\n        adam_beta1=0.9,\n        adam_beta2=0.98,\n        adam_epsilon=1e-8,\n        warmup_steps=0.03,\n        fp16=True,\n        seed=42,\n        save_total_limit=1,\n    ),data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.config.use_cache = False\ntrainer.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_model = \"gemma-model\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.model.save_pretrained(\"gemma-model\")# save the model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Save the model weights and adapters","metadata":{}},{"cell_type":"code","source":"base_model = AutoModelForCausalLM.from_pretrained(\n    checkpoint,\n    low_cpu_mem_usage=True,\n    return_dict=True,\n    torch_dtype=torch.float16,\n    device_map={\"\": 0},\n)\nmerged_model= PeftModel.from_pretrained(base_model, new_model)\nmerged_model= merged_model.merge_and_unload()\n\n# Save the merged model\nmerged_model.save_pretrained(\"merged_model\",safe_serialization=True)\ntokenizer.save_pretrained(\"merged_model\")\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tuned_model = AutoModelForCausalLM.from_pretrained(\"/kaggle/working/gemma_model\")# load the model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Test With New Samples","metadata":{}},{"cell_type":"code","source":"prompt_template = \"\"\"\n<start_of_turn>user\nYou are an intelligent AI specialized in generating SQL queries.\nYour task is to assist users in formulating SQL queries to retrieve specific information from a database.\nPlease provide the SQL query corresponding to the given prompt and context:\n\nPrompt:\nWhat is the total production of oil from the onshore fields in the Beaufort Sea?\n\nContext:\nCREATE TABLE beaufort_sea_oil_production (field VARCHAR(255), year INT, production FLOAT);<end_of_turn>\n<start_of_turn>model\n    \"\"\"\n\nprompt = prompt_template\nencodeds = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True).input_ids\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ntuned_model.to(device)\ninputs = encodeds.to(device)\n\n\n# Increase max_new_tokens if needed\ngenerated_ids = tuned_model.generate(inputs, max_new_tokens=1000, do_sample=True, temperature = 0.7,pad_token_id=tokenizer.eos_token_id)\nans = ''\nfor i in tokenizer.decode(generated_ids[0], skip_special_tokens=True).split('<end_of_turn>')[:2]:\n    ans += i\n\n# Extract only the model's answer\nmodel_answer = ans.split(\"model\")[1].strip()\nprint(model_answer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}